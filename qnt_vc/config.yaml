dataset:
  process:
    batch_size: 2
    num_workers: 1
  random_select: 0.1
  train:
    source_path: 'audio/vc/qnt_jnas_train.csv'
    target_path: 'audio/vc/qnt_deaf_train.csv'
    speaker_path: 'audio/vc/speakers.json'
  valid:  
    source_path: 'audio/vc/qnt_jnas_valid.csv'
    target_path: 'audio/vc/qnt_deaf_valid.csv'
    speaker_path: 'audio/vc/speakers.json'
  test:
    csv_path: 
    enroll_path: 
    noise_path:
trainer:
  accelerator: 'auto'
  accumulate_grad_batches: 8 #128
  max_epochs: 400
  precision: '16-mixed'
  profiler: 'simple'
  gradient_clip_val: 5.
optimizer:
  lr: 1.e-5
logger:
  save_dir: './log_qnt_vc'
  version: 4
  name: 'lightning_logs'
checkpoint:
  monitor: 'valid_loss'
  filename: 'checkpoint_{epoch}-{step}-{valid_loss:.3f}'
  save_last: True
  save_top_k: 1
  mode: 'min'
  every_n_epochs: 1
# models
transformer:
  num_speakers: 338
  num_speaker_embeddings: 384
  num_qnt_symbols: 1026
  num_symbol_embeddings: 384
  num_channels: 8
  max_length: 2048
  num_heads: 16
  num_encoder_layers: 8
  num_decoder_layers: 8
  num_nar_layers: 12
  dim_feedforward: 2048
  dropout: 0.1
